{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os import environ\n",
    "environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _emphasis: boolean (nullable = true)\n",
      " |-- _id: long (nullable = true)\n",
      " |-- time: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _id: string (nullable = true)\n",
      " |    |    |-- _value: string (nullable = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |-- w: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _emphasis: boolean (nullable = true)\n",
      " |    |    |-- _id: double (nullable = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the subtitle file and transform it into a DataFrame\n",
    "df = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='document',rowTag='s').option(\"valueTag\", \"content\").load(DATA_DIR+'6653249.xml')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                time|          word|\n",
      "+--------------------+--------------+\n",
      "|[00:01:40,634, 00...|      Terminal|\n",
      "|[00:01:40,634, 00...|             A|\n",
      "|[00:01:40,634, 00...|            of|\n",
      "|[00:01:40,634, 00...|        Boston|\n",
      "|[00:01:40,634, 00...| international|\n",
      "|[00:01:40,634, 00...|       airport|\n",
      "|[00:01:40,634, 00...|             .|\n",
      "|[00:01:43,670, 00...|Transportation|\n",
      "|[00:01:43,670, 00...|       between|\n",
      "|[00:01:43,670, 00...|     terminals|\n",
      "|[00:01:43,670, 00...|           ...|\n",
      "|[00:01:53,612, 00...|          Take|\n",
      "|[00:01:53,612, 00...|          your|\n",
      "|[00:01:53,612, 00...|        laptop|\n",
      "|[00:01:53,612, 00...|           out|\n",
      "|[00:01:53,612, 00...|            of|\n",
      "|[00:01:53,612, 00...|          your|\n",
      "|[00:01:53,612, 00...|           bag|\n",
      "|[00:01:53,612, 00...|             .|\n",
      "|[00:02:14,264, 00...|           The|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We select only time and w columns, keeping only the (word) value of w\n",
    "data = df.select(col('time._value').alias('time'),explode('w.content').alias('word'))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|startingTime|          word|\n",
      "+------------+--------------+\n",
      "|00:01:40,634|      Terminal|\n",
      "|00:01:40,634|             A|\n",
      "|00:01:40,634|            of|\n",
      "|00:01:40,634|        Boston|\n",
      "|00:01:40,634| international|\n",
      "|00:01:40,634|       airport|\n",
      "|00:01:40,634|             .|\n",
      "|00:01:43,670|Transportation|\n",
      "|00:01:43,670|       between|\n",
      "|00:01:43,670|     terminals|\n",
      "|00:01:43,670|           ...|\n",
      "|00:01:53,612|          Take|\n",
      "|00:01:53,612|          your|\n",
      "|00:01:53,612|        laptop|\n",
      "|00:01:53,612|           out|\n",
      "|00:01:53,612|            of|\n",
      "|00:01:53,612|          your|\n",
      "|00:01:53,612|           bag|\n",
      "|00:01:53,612|             .|\n",
      "|00:02:14,264|           The|\n",
      "+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We keep the starting time of the subtitle only\n",
    "clean_data = data.withColumn('startingTime',data['time'].getItem(0)).select(col('startingTime'), col('word'))\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "+------------+--------------+\n",
      "|startingTime|          word|\n",
      "+------------+--------------+\n",
      "|00:01:40,634|      Terminal|\n",
      "|00:01:40,634|        Boston|\n",
      "|00:01:40,634| international|\n",
      "|00:01:40,634|       airport|\n",
      "|00:01:43,670|Transportation|\n",
      "|00:01:43,670|     terminals|\n",
      "|00:01:53,612|          Take|\n",
      "|00:01:53,612|        laptop|\n",
      "|00:01:53,612|           bag|\n",
      "|00:02:14,264|        Boston|\n",
      "|00:02:14,264|        police|\n",
      "|00:02:14,264|    department|\n",
      "|00:02:14,264|      requests|\n",
      "|00:02:14,264|        safety|\n",
      "|00:02:14,264|      security|\n",
      "|00:02:14,264|           per|\n",
      "|00:02:14,264|           FAA|\n",
      "|00:02:14,264|   regulations|\n",
      "|00:02:41,656|           'll|\n",
      "|00:02:41,656|          home|\n",
      "+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We filter out the stop words and punctuation\n",
    "#We use the stop-words list from NLTK\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "\n",
    "#Checks whether a word is a stop-word or a sequence of non-alphabetic characters and sets them to None\n",
    "def isAWord(x):\n",
    "    if(len(x)==0 or x.lower() in stopWords or not any(c.isalpha() for c in x)):\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "udf_is_a_word = udf(isAWord, StringType())\n",
    "\n",
    "newData = clean_data.select(('startingTime'),udf_is_a_word('word').alias('word'))\n",
    "cleanData = newData.na.drop(subset=[\"word\"])\n",
    "cleanData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+----------+------------+--------+\n",
      "|blocks|cds|confidence|      date|    duration|language|\n",
      "+------+---+----------+----------+------------+--------+\n",
      "|   870|1/1|       1.0|2016-06-10|01:32:10,143| English|\n",
      "+------+---+----------+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='metadata',rowTag='subtitle').load(DATA_DIR+'6653249.xml')\n",
    "metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only use the date of the subtitles therefore we define a method to just get this one single value as a string, which we'll later process according to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDate(filename):\n",
    "    metadata = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='metadata',rowTag='subtitle').load(DATA_DIR+filename)\n",
    "    date = metadata.select('date').head()[0]\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-06-10'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDate('6653249.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using four of the IMDB datasets: \n",
    "    1. basics - for various metadata about the movie/tv series\n",
    "    2. episode - to match the subtitles tv series episodes with the overall series in the database\n",
    "    3. principals - for the character name info\n",
    "    4. ratings - for the ratings and views info\n",
    "All of them are described here: https://www.imdb.com/interfaces/\n",
    "All of the datasets fit on our local machines, however we'll need to use the cluster when joining this data with the subtitle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading datasets\n",
    "\n",
    "basics = spark.read.csv(DATA_DIR+'title.basics.tsv',sep='\\t',header=True)\n",
    "episode = spark.read.csv(DATA_DIR+'title.episode.tsv',sep='\\t',header=True)\n",
    "principals = spark.read.csv(DATA_DIR+'title.principals.tsv',sep='\\t',header=True)\n",
    "ratings = spark.read.csv(DATA_DIR+'title.ratings.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the following fields from every table:\n",
    "    - basics:\n",
    "        - all fields withouth isAdult, originalTitle\n",
    "    - episode:\n",
    "        - all fields\n",
    "    - principals:\n",
    "        - tconst, ordering, nconst, characters\n",
    "    - ratings:\n",
    "        - all fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   titleType|\n",
      "+------------+\n",
      "|    tvSeries|\n",
      "|tvMiniSeries|\n",
      "|     tvMovie|\n",
      "|   tvEpisode|\n",
      "|       movie|\n",
      "|   tvSpecial|\n",
      "|       video|\n",
      "|   videoGame|\n",
      "|     tvShort|\n",
      "|       short|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basics.select('titleType').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not interested in the videoGame, tvShort or short types fo we get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the originalTitle and isAdult columns\n",
    "basics_clean = basics.drop('isAdult', 'originalTitle').filter(col('titleType')!= 'short').filter(col('titleType')!= 'videoGame').filter(col('titleType')!= 'tvShort')\n",
    "\n",
    "#remove the rows where the category of a person is not 'actor' and where the character name is empty\n",
    "#remove the category and job columns, since category is always 'actor' now\n",
    "principals_clean = principals.drop('category','job').filter(col('category')!='actor').filter(col('characters') != '\\\\N')\n",
    "\n",
    "#remove episodes with empty series or episode numbers\n",
    "episode_clean = episode.filter(col('seasonNumber')!='\\\\N').filter(col('episodeNumber')!='\\\\N')\n",
    "ratings_clean = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|primaryTitle|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+------------+---------+-------+--------------+--------------------+\n",
      "|tt0098936| tvSeries|  Twin Peaks|     1990|   1991|            47| Crime,Drama,Mystery|\n",
      "|tt0186641|    video|  Twin Peaks|     1993|     \\N|            \\N|               Adult|\n",
      "|tt2395641|tvEpisode|  Twin Peaks|     2006|     \\N|            \\N|         Documentary|\n",
      "|tt2650780|tvEpisode|  Twin Peaks|     2013|     \\N|            44|Adventure,Reality-TV|\n",
      "|tt3225942|tvEpisode|  Twin Peaks|     2013|     \\N|            43|          Reality-TV|\n",
      "|tt3421958|tvEpisode|  Twin Peaks|     2010|     \\N|            \\N|         Drama,Short|\n",
      "|tt4093826| tvSeries|  Twin Peaks|     2017|   2017|            60| Crime,Drama,Fantasy|\n",
      "+---------+---------+------------+---------+-------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note that we leave the endYear column because it is valid for the TV Series, e.x.\n",
    "basics_clean.filter(col('primaryTitle')=='Twin Peaks').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+---------+-------+--------------+--------------------+\n",
      "|tt0000009|    movie|          Miss Jerry|     1894|     \\N|            45|             Romance|\n",
      "|tt0000147|    movie|The Corbett-Fitzs...|     1897|     \\N|            20|Documentary,News,...|\n",
      "|tt0000335|    movie|Soldiers of the C...|     1900|     \\N|            \\N|     Biography,Drama|\n",
      "|tt0000502|    movie|            Bohemios|     1905|     \\N|           100|                  \\N|\n",
      "|tt0000574|    movie|The Story of the ...|     1906|     \\N|            70|Biography,Crime,D...|\n",
      "|tt0000615|    movie|  Robbery Under Arms|     1907|     \\N|            \\N|               Drama|\n",
      "|tt0000630|    movie|              Hamlet|     1908|     \\N|            \\N|               Drama|\n",
      "|tt0000675|    movie|         Don Quijote|     1908|     \\N|            \\N|               Drama|\n",
      "|tt0000676|    movie|Don Álvaro o la f...|     1908|     \\N|            \\N|               Drama|\n",
      "|tt0000679|    movie|The Fairylogue an...|     1908|     \\N|           120|   Adventure,Fantasy|\n",
      "|tt0000739|    movie|El pastorcito de ...|     1908|     \\N|            \\N|               Drama|\n",
      "|tt0000793|    movie|       Andreas Hofer|     1909|     \\N|            \\N|               Drama|\n",
      "|tt0000812|    movie|   El blocao Velarde|     1909|     \\N|            \\N|                  \\N|\n",
      "|tt0000814|    movie|La bocana de Mar ...|     1909|     \\N|            \\N|                  \\N|\n",
      "|tt0000838|    movie|  A Cultura do Cacau|     1909|     \\N|            \\N|                  \\N|\n",
      "|tt0000842|    movie|De Garraf a Barce...|     1909|     \\N|            \\N|                  \\N|\n",
      "|tt0000846|    movie|Un día en Xochimilco|     1909|     \\N|            \\N|                  \\N|\n",
      "|tt0000850|    movie|    Los dos hermanos|     1909|     \\N|            \\N|                  \\N|\n",
      "|tt0000859|    movie|Fabricación del c...|     1909|     \\N|            \\N|                  \\N|\n",
      "|tt0000862|    movie|          Faldgruben|     1909|     \\N|            \\N|                  \\N|\n",
      "+---------+---------+--------------------+---------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basics_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+--------------------+\n",
      "|   tconst|ordering|   nconst|          characters|\n",
      "+---------+--------+---------+--------------------+\n",
      "|tt0000001|       1|nm1588970|         [\"Herself\"]|\n",
      "|tt0000009|       1|nm0063086|[\"Miss Geraldine ...|\n",
      "|tt0000009|       3|nm1309758|[\"Himself - the D...|\n",
      "|tt0000012|       1|nm2880396|         [\"Herself\"]|\n",
      "|tt0000012|       2|nm9735580|         [\"Himself\"]|\n",
      "|tt0000012|       3|nm0525900|         [\"Herself\"]|\n",
      "|tt0000012|       4|nm9735581|         [\"Herself\"]|\n",
      "|tt0000012|       7|nm9735579|         [\"Herself\"]|\n",
      "|tt0000012|       8|nm9653419|         [\"Herself\"]|\n",
      "|tt0000013|       1|nm0525908|         [\"Himself\"]|\n",
      "|tt0000013|       2|nm1715062|         [\"Himself\"]|\n",
      "|tt0000016|       1|nm0525900|[\"Herself (on the...|\n",
      "|tt0000016|       2|nm9735581|[\"Herself (on the...|\n",
      "|tt0000017|       2|nm3692829|        [\"The girl\"]|\n",
      "|tt0000024|       1|nm0256651|[\"Herself - Empre...|\n",
      "|tt0000024|       2|nm0435118|[\"Himself - Emper...|\n",
      "|tt0000028|       1|nm2350838|         [\"Himself\"]|\n",
      "|tt0000028|       2|nm0525908|         [\"Himself\"]|\n",
      "|tt0000029|       1|nm0525908|         [\"Himself\"]|\n",
      "|tt0000029|       2|nm0525900|         [\"Herself\"]|\n",
      "+---------+--------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "principals_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+-------------+\n",
      "|   tconst|parentTconst|seasonNumber|episodeNumber|\n",
      "+---------+------------+------------+-------------+\n",
      "|tt0041951|   tt0041038|           1|            9|\n",
      "|tt0042816|   tt0989125|           1|           17|\n",
      "|tt0043426|   tt0040051|           3|           42|\n",
      "|tt0043631|   tt0989125|           2|           16|\n",
      "|tt0043693|   tt0989125|           2|            8|\n",
      "|tt0043710|   tt0989125|           3|            3|\n",
      "|tt0044093|   tt0959862|           1|            6|\n",
      "|tt0044901|   tt0989125|           3|           46|\n",
      "|tt0045519|   tt0989125|           4|           11|\n",
      "|tt0045960|   tt0044284|           2|            3|\n",
      "|tt0046135|   tt0989125|           4|            5|\n",
      "|tt0046855|   tt0046643|           1|            4|\n",
      "|tt0046864|   tt0989125|           5|           20|\n",
      "|tt0047810|   tt0914702|           3|           36|\n",
      "|tt0047852|   tt0047745|           1|           15|\n",
      "|tt0047858|   tt0046637|           2|            9|\n",
      "|tt0047961|   tt0989125|           6|            5|\n",
      "|tt0048067|   tt0046587|           2|           20|\n",
      "|tt0048302|   tt0047768|           1|            6|\n",
      "|tt0048371|   tt0989125|           6|           11|\n",
      "+---------+------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episode_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.8|    1440|\n",
      "|tt0000002|          6.3|     172|\n",
      "|tt0000003|          6.6|    1041|\n",
      "|tt0000004|          6.4|     102|\n",
      "|tt0000005|          6.2|    1735|\n",
      "|tt0000006|          5.5|      91|\n",
      "|tt0000007|          5.5|     579|\n",
      "|tt0000008|          5.6|    1539|\n",
      "|tt0000009|          5.6|      74|\n",
      "|tt0000010|          6.9|    5127|\n",
      "|tt0000011|          5.4|     214|\n",
      "|tt0000012|          7.4|    8599|\n",
      "|tt0000013|          5.7|    1318|\n",
      "|tt0000014|          7.2|    3739|\n",
      "|tt0000015|          6.2|     660|\n",
      "|tt0000016|          5.9|     982|\n",
      "|tt0000017|          4.8|     197|\n",
      "|tt0000018|          5.5|     414|\n",
      "|tt0000019|          6.6|      13|\n",
      "|tt0000020|          5.1|     232|\n",
      "+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names, cities, words and themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the Datamuse API (http://www.datamuse.com/api/) to get the word frequency in order to spot well know or unusual words as well as words related to a given topic (a theme in our case). We define two functions to ease this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:289.446847\n",
      "['bloom', 'blossom', 'efflorescence', 'peak', 'flush']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "#returns a frequency of a word per 1 000 000 words of English text\n",
    "def frequency(word):\n",
    "    call = 'https://api.datamuse.com/words?sp=%s&qe=sp&md=fr'%word\n",
    "    response = requests.get(call)\n",
    "    data = response.json()\n",
    "    return data.pop()['tags'][2]\n",
    "\n",
    "#returns 5 words related to the word\n",
    "def topic(word):\n",
    "    call = 'https://api.datamuse.com/words?topics=%s'%word\n",
    "    response = requests.get(call)\n",
    "    data = response.json()\n",
    "    result = []\n",
    "    for row in data[:5]:\n",
    "        result.append(row['word'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:289.446847\n"
     ]
    }
   ],
   "source": [
    "print(frequency('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bloom', 'blossom', 'efflorescence', 'peak', 'flush']\n"
     ]
    }
   ],
   "source": [
    "print(topic('flower'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the cities, we'll use the geotext library that will help us filter out the cities from the subitle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geotext import GeoText\n",
    "\n",
    "def isCity(word):\n",
    "    places = GeoText(word)\n",
    "    return len(places.cities)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isCity(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isCity(\"asdfghj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for parsing names of the characters properly we will use the nameparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nameparser import HumanName\n",
    "\n",
    "def firstName(inputName):\n",
    "    name = HumanName(inputName)\n",
    "    return name['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jules\n"
     ]
    }
   ],
   "source": [
    "print(firstName(\"Jules David Winnifield III\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
