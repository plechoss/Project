{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os import environ\n",
    "environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _emphasis: boolean (nullable = true)\n",
      " |-- _id: long (nullable = true)\n",
      " |-- time: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _id: string (nullable = true)\n",
      " |    |    |-- _value: string (nullable = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |-- w: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _emphasis: boolean (nullable = true)\n",
      " |    |    |-- _id: double (nullable = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the subtitle file and transform it into a DataFrame\n",
    "df = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='document',rowTag='s').option(\"valueTag\", \"content\").load(DATA_DIR+'6653249.xml')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                time|          word|\n",
      "+--------------------+--------------+\n",
      "|[00:01:40,634, 00...|      Terminal|\n",
      "|[00:01:40,634, 00...|             A|\n",
      "|[00:01:40,634, 00...|            of|\n",
      "|[00:01:40,634, 00...|        Boston|\n",
      "|[00:01:40,634, 00...| international|\n",
      "|[00:01:40,634, 00...|       airport|\n",
      "|[00:01:40,634, 00...|             .|\n",
      "|[00:01:43,670, 00...|Transportation|\n",
      "|[00:01:43,670, 00...|       between|\n",
      "|[00:01:43,670, 00...|     terminals|\n",
      "|[00:01:43,670, 00...|           ...|\n",
      "|[00:01:53,612, 00...|          Take|\n",
      "|[00:01:53,612, 00...|          your|\n",
      "|[00:01:53,612, 00...|        laptop|\n",
      "|[00:01:53,612, 00...|           out|\n",
      "|[00:01:53,612, 00...|            of|\n",
      "|[00:01:53,612, 00...|          your|\n",
      "|[00:01:53,612, 00...|           bag|\n",
      "|[00:01:53,612, 00...|             .|\n",
      "|[00:02:14,264, 00...|           The|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We select only time and w columns, keeping only the (word) value of w\n",
    "data = df.select(col('time._value').alias('time'),explode('w.content').alias('word'))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|startingTime|          word|\n",
      "+------------+--------------+\n",
      "|00:01:40,634|      Terminal|\n",
      "|00:01:40,634|             A|\n",
      "|00:01:40,634|            of|\n",
      "|00:01:40,634|        Boston|\n",
      "|00:01:40,634| international|\n",
      "|00:01:40,634|       airport|\n",
      "|00:01:40,634|             .|\n",
      "|00:01:43,670|Transportation|\n",
      "|00:01:43,670|       between|\n",
      "|00:01:43,670|     terminals|\n",
      "|00:01:43,670|           ...|\n",
      "|00:01:53,612|          Take|\n",
      "|00:01:53,612|          your|\n",
      "|00:01:53,612|        laptop|\n",
      "|00:01:53,612|           out|\n",
      "|00:01:53,612|            of|\n",
      "|00:01:53,612|          your|\n",
      "|00:01:53,612|           bag|\n",
      "|00:01:53,612|             .|\n",
      "|00:02:14,264|           The|\n",
      "+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We keep the starting time of the subtitle only\n",
    "clean_data = data.withColumn('startingTime',data['time'].getItem(0)).select(col('startingTime'), col('word'))\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "+------------+--------------+\n",
      "|startingTime|          word|\n",
      "+------------+--------------+\n",
      "|00:01:40,634|      Terminal|\n",
      "|00:01:40,634|        Boston|\n",
      "|00:01:40,634| international|\n",
      "|00:01:40,634|       airport|\n",
      "|00:01:43,670|Transportation|\n",
      "|00:01:43,670|     terminals|\n",
      "|00:01:53,612|          Take|\n",
      "|00:01:53,612|        laptop|\n",
      "|00:01:53,612|           bag|\n",
      "|00:02:14,264|        Boston|\n",
      "|00:02:14,264|        police|\n",
      "|00:02:14,264|    department|\n",
      "|00:02:14,264|      requests|\n",
      "|00:02:14,264|        safety|\n",
      "|00:02:14,264|      security|\n",
      "|00:02:14,264|           per|\n",
      "|00:02:14,264|           FAA|\n",
      "|00:02:14,264|   regulations|\n",
      "|00:02:41,656|           'll|\n",
      "|00:02:41,656|          home|\n",
      "+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We filter out the stop words and punctuation\n",
    "#We use the stop-words list from NLTK\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "\n",
    "#Checks whether a word is a stop-word or a sequence of non-alphabetic characters and sets them to None\n",
    "def isAWord(x):\n",
    "    if(len(x)==0 or x.lower() in stopWords or not any(c.isalpha() for c in x)):\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "udf_is_a_word = udf(isAWord, StringType())\n",
    "\n",
    "newData = clean_data.select(('startingTime'),udf_is_a_word('word').alias('word'))\n",
    "cleanData = newData.na.drop(subset=[\"word\"])\n",
    "cleanData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+----------+------------+--------+\n",
      "|blocks|cds|confidence|      date|    duration|language|\n",
      "+------+---+----------+----------+------------+--------+\n",
      "|   870|1/1|       1.0|2016-06-10|01:32:10,143| English|\n",
      "+------+---+----------+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='metadata',rowTag='subtitle').load(DATA_DIR+'6653249.xml')\n",
    "metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only use the date of the subtitles therefore we define a method to just get this one single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDate(filename):\n",
    "    metadata = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='metadata',rowTag='subtitle').load(DATA_DIR+filename)\n",
    "    date = metadata.select('date').head()[0]\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-06-10'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDate('6653249.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using four of the IMDB datasets: \n",
    "    1. basics - for various metadata about the movie/tv series\n",
    "    2. episode - to match the subtitles tv series episodes with the overall series in the database\n",
    "    3. principals - for the character name info\n",
    "    4. ratings - for the ratings and views info\n",
    "All of them are described here: https://www.imdb.com/interfaces/\n",
    "All of the datasets fit on our local machines, however we'll need to use the cluster when joining this data with the subtitle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading datasets\n",
    "\n",
    "basics = spark.read.csv(DATA_DIR+'title.basics.tsv',sep='\\t',header=True)\n",
    "episode = spark.read.csv(DATA_DIR+'title.episode.tsv',sep='\\t',header=True)\n",
    "principals = spark.read.csv(DATA_DIR+'title.principals.tsv',sep='\\t',header=True)\n",
    "ratings = spark.read.csv(DATA_DIR+'title.ratings.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the following fields from every table:\n",
    "    - basics:\n",
    "        - all fields withouth isAdult, originalTitle\n",
    "    - episode:\n",
    "        - all fields\n",
    "    - principals:\n",
    "        - tconst, ordering, nconst, characters\n",
    "    - ratings:\n",
    "        - all fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the originalTitle and isAdult columns\n",
    "basics_clean = basics.drop('isAdult', 'originalTitle')\n",
    "\n",
    "#remove the rows where the category of a person is not 'actor' and where the character name is empty\n",
    "#remove the category and job columns, since category is always 'actor' now\n",
    "principals_clean = principals.drop('category','job').filter(col('category')!='actor').filter(col('characters') != '\\\\N')\n",
    "\n",
    "episode_clean = episode\n",
    "ratings_clean = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|     1892|     \\N|             5|     Animation,Short|\n",
      "|tt0000003|    short|      Pauvre Pierrot|     1892|     \\N|             4|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|     1892|     \\N|            \\N|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|     1893|     \\N|             1|        Comedy,Short|\n",
      "|tt0000006|    short|   Chinese Opium Den|     1894|     \\N|             1|               Short|\n",
      "|tt0000007|    short|Corbett and Court...|     1894|     \\N|             1|         Short,Sport|\n",
      "|tt0000008|    short|Edison Kinetoscop...|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000009|    movie|          Miss Jerry|     1894|     \\N|            45|             Romance|\n",
      "|tt0000010|    short|Employees Leaving...|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000011|    short|Akrobatisches Pot...|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000012|    short|The Arrival of a ...|     1896|     \\N|             1|   Documentary,Short|\n",
      "|tt0000013|    short|The Photographica...|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000014|    short|Tables Turned on ...|     1895|     \\N|             1|        Comedy,Short|\n",
      "|tt0000015|    short| Autour d'une cabine|     1894|     \\N|             2|     Animation,Short|\n",
      "|tt0000016|    short|Barque sortant du...|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000017|    short|Italienischer Bau...|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000018|    short|Das boxende KÃ¤nguruh|     1895|     \\N|             1|               Short|\n",
      "|tt0000019|    short|    The Clown Barber|     1898|     \\N|            \\N|        Comedy,Short|\n",
      "|tt0000020|    short|      The Derby 1895|     1895|     \\N|             1|Documentary,Short...|\n",
      "+---------+---------+--------------------+---------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note that we leave the endYear column because it is valid for the TV Series\n",
    "basics_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+--------------------+\n",
      "|   tconst|ordering|   nconst|          characters|\n",
      "+---------+--------+---------+--------------------+\n",
      "|tt0000001|       1|nm1588970|         [\"Herself\"]|\n",
      "|tt0000009|       1|nm0063086|[\"Miss Geraldine ...|\n",
      "|tt0000009|       3|nm1309758|[\"Himself - the D...|\n",
      "|tt0000012|       1|nm2880396|         [\"Herself\"]|\n",
      "|tt0000012|       2|nm9735580|         [\"Himself\"]|\n",
      "|tt0000012|       3|nm0525900|         [\"Herself\"]|\n",
      "|tt0000012|       4|nm9735581|         [\"Herself\"]|\n",
      "|tt0000012|       7|nm9735579|         [\"Herself\"]|\n",
      "|tt0000012|       8|nm9653419|         [\"Herself\"]|\n",
      "|tt0000013|       1|nm0525908|         [\"Himself\"]|\n",
      "|tt0000013|       2|nm1715062|         [\"Himself\"]|\n",
      "|tt0000016|       1|nm0525900|[\"Herself (on the...|\n",
      "|tt0000016|       2|nm9735581|[\"Herself (on the...|\n",
      "|tt0000017|       2|nm3692829|        [\"The girl\"]|\n",
      "|tt0000024|       1|nm0256651|[\"Herself - Empre...|\n",
      "|tt0000024|       2|nm0435118|[\"Himself - Emper...|\n",
      "|tt0000028|       1|nm2350838|         [\"Himself\"]|\n",
      "|tt0000028|       2|nm0525908|         [\"Himself\"]|\n",
      "|tt0000029|       1|nm0525908|         [\"Himself\"]|\n",
      "|tt0000029|       2|nm0525900|         [\"Herself\"]|\n",
      "+---------+--------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "principals_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+-------------+\n",
      "|   tconst|parentTconst|seasonNumber|episodeNumber|\n",
      "+---------+------------+------------+-------------+\n",
      "|tt0041951|   tt0041038|           1|            9|\n",
      "|tt0042816|   tt0989125|           1|           17|\n",
      "|tt0042889|   tt0989125|          \\N|           \\N|\n",
      "|tt0043426|   tt0040051|           3|           42|\n",
      "|tt0043631|   tt0989125|           2|           16|\n",
      "|tt0043693|   tt0989125|           2|            8|\n",
      "|tt0043710|   tt0989125|           3|            3|\n",
      "|tt0044093|   tt0959862|           1|            6|\n",
      "|tt0044901|   tt0989125|           3|           46|\n",
      "|tt0045519|   tt0989125|           4|           11|\n",
      "|tt0045960|   tt0044284|           2|            3|\n",
      "|tt0046135|   tt0989125|           4|            5|\n",
      "|tt0046150|   tt0341798|          \\N|           \\N|\n",
      "|tt0046855|   tt0046643|           1|            4|\n",
      "|tt0046864|   tt0989125|           5|           20|\n",
      "|tt0047810|   tt0914702|           3|           36|\n",
      "|tt0047852|   tt0047745|           1|           15|\n",
      "|tt0047858|   tt0046637|           2|            9|\n",
      "|tt0047961|   tt0989125|           6|            5|\n",
      "|tt0048067|   tt0046587|           2|           20|\n",
      "+---------+------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episode_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
